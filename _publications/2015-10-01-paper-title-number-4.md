---
title: ü§π‚Äç‚ôÇÔ∏è Multimodal Multitask Emotion Recognition using Images, Texts and Tags
collection: publications
permalink: /publication/2019-06-05-Multimodal-Multitask-Emotion
excerpt: 'In this work, we explore the use of images, texts and tags for emotion recognition. However, using several modalities can also come with an additional challenge that is often ignored, namely the problem of "missing modality". We propose a multimodal model that leverages a multitask framework to enable the use of training data composed of an arbitrary number of modality, while it can also perform predictions with missing modalities.'
date: 2019-06-05
venue: 'Workshop on Crossmodal Learning and Application'
#paperurl: 'https://pdfs.semanticscholar.org/2216/70f781b6e1796ad099d990051d85aa4a24d3.pdf'
#citation: 'Pag√© Fortin, M., & Chaib-draa, B. (2019, June). Multimodal multitask emotion recognition using images, texts and tags. In Proceedings of the <i>ACM Workshop on Crossmodal Learning and Application</i> (pp. 3-10).'
header:
    teaser: ../images/publications/wcrml.png
---
Recently, multimodal emotion recognition received an increasing
interest due to its potential to improve performance by leveraging
complementary sources of information. In this work, we explore
the use of images, texts and tags for emotion recognition. However,
using several modalities can also come with an additional challenge
that is often ignored, namely the problem of ‚Äúmissing modality‚Äù.
Social media users do not always publish content containing an
image, text and tags, and consequently one or two modalities are
often missing at test time. Similarly, the labeled training data that
contain all modalities can be limited.

Taking this in consideration, we propose a multimodal model
that leverages a multitask framework to enable the use of training
data composed of an arbitrary number of modality, while it can
also perform predictions with missing modalities. We show that
our approach is robust to one or two missing modalities at test
time. Also, with this framework it becomes easy to fine-tune some
parts of our model with unimodal and bimodal training data, which
can further improve overall performance. Finally, our experiments
support that this multitask learning also acts as a regularization
mechanism that improves generalization.

[Download paper here](https://www.researchgate.net/profile/Mathieu-Page-Fortin/publication/333695810_Multimodal_Multitask_Emotion_Recognition_using_Images_Texts_and_Tags/links/61e6f1adc5e3103375a36bff/Multimodal-Multitask-Emotion-Recognition-using-Images-Texts-and-Tags.pdf)

Recommended citation: Pag√© Fortin, M., & Chaib-draa, B. (2019, June). Multimodal multitask emotion recognition using images, texts and tags. In Proceedings of the <i>ACM Workshop on Crossmodal Learning and Application</i> (pp. 3-10).