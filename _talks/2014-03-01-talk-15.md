---
title: "ðŸ§± Compositionality and Contextuality in Multimodal Deep Learning"
collection: talks
type: "Research Proposal"
permalink: /talks/2020-05-28-Research-Proposal
venue: "Laval University"
date: 2020-05-28
location: "Canada"
---
Convolutional networks have been very successful in the last decade in digital vision. Generally, these networks are able to learn good quality representations due to their compositional nature, that is, they are able to learn representations of higher level of abstraction with each layer of convolution. While it is certain that convolutional networks are compositional in the mathematical sense, recent work nevertheless question the hypothesis that these networks are conceptually compositional in the sense that they would learn to compose high-level concepts. On the contrary, convolutional networks would rather learn to detect surface textures on images, which could explain their vulnerability to conflicting examples, their difficulty in generalizing to data outside the training distribution, their need for a large number of training data, and many more.

In this research project proposal, we hypothesize that convolutional networks would benefit from using semantic concepts more explicitly. More specifically, we propose to investigate two important principles in semantic theory: the principles of contextuality and compositionality. This would first be articulated by an approach which uses the context of the scene in which an object appears in order to improve the representation of its class, and thus, facilitate learning from few examples. For compositionality, we propose to explore methods of disentangled representation learning that use images and text descriptions to guide the learning of higher-level concepts.

[See presentation](https://drive.google.com/file/d/1PS-zPVb9fM9cvFLFKlFF0NGmHcw92Y2e/view?usp=sharing)

